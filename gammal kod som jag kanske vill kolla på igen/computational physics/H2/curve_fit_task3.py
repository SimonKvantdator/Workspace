import numpy as np
import scipy
from scipy import stats
import matplotlib as mpl
from matplotlib import pyplot as plt
import corner
from emcee import EnsembleSampler

### change matplotlib default fontsize & such ###
mpl.rc('font', family='sans-serif') 
mpl.rc('font', serif='Helvetica Neue') 
mpl.rc('text', usetex='false') 
mpl.rcParams.update({'font.size': 16})


### read binary files generated by C ###
alphas = np.fromfile("alphas_task3.bin")
energies = np.fromfile("energies_task3.bin") # energies[walker_index + alpha_index * nbr_walkers
variances = np.fromfile("variances_task3.bin") # variances[walker_index + alpha_index * nbr_walkers
parameters = np.fromfile("parameters_task1.bin") # {alpha, Delta, nbr_samples, nbr_walkers}

### extract parameters ###
nbr_samples = int(parameters[2])
nbr_walkers = int(parameters[3])
alphas_length = len(alphas)
print(f'nbr_samples = {nbr_samples}')
print(f'nbr_walkers = {nbr_walkers}')

### take average over walkers & extract relevant info ###
mean_energies = np.array([np.mean(energies[nbr_walkers * i:nbr_walkers * (i+1)]) for i in range(alphas_length)])
mean_variances = np.array([np.mean(variances[nbr_walkers * i:nbr_walkers * (i+1)]) for i in range(alphas_length)])
statistical_inefficiency = 8
sigmas = (1 / np.sqrt(nbr_walkers)) * np.sqrt(mean_variances / (nbr_samples / statistical_inefficiency))

widths = sigmas
y_vec = mean_energies
x_vec = alphas


### define 3rd degree polynomial model ###
def predict_y(x, theta):
    return theta[0] + theta[1] * x + theta[2] * x**2 + theta[3] * x**3

### define pdf:s ###
def log_normal(x, center=0.0, width=1.0):
    """
    Returns log of normal pdf at points x.
    Not normalized.
    center is mu & width is sigma.
    """
    
    t = (x - center) / width
    return -np.log(width) - t**2 / 2

def log_prior(theta):
    return 0. # flat prior

def log_likelihood(theta):
    
    centers = predict_y(x_vec, theta) # we now instead think of the distribution centered at y=predict_y(x)
    likelihoods = [] # initiate
    for y, center, width in zip(y_vec, centers, widths):        
        y_dist = lambda t : log_normal(t, center=center, width=width)
        likelihoods.append(y_dist(y))

    return np.sum(likelihoods)

def log_posterior(theta):
    return log_prior(theta) + log_likelihood(theta)


### initiate MCMC sampler
dim = 4 # we use a 3rd degree polynomial model
nbr_bins = 31
nbr_walkers = 32
nbr_warmup = 400
nbr_samples = 10000

initial_positions = 2.0 * (np.random.rand(nbr_walkers, dim) - 0.5)
sampler = EnsembleSampler(nbr_walkers, dim, log_posterior)

### run sampler ###
pos, _, _ = sampler.run_mcmc(initial_positions, nbr_warmup)
sampler.reset()
sampler.run_mcmc(pos, nbr_samples)
samples = sampler.flatchain

### make histogram corner plot ###
latex_labels = [r"$\theta_0$", r"$\theta_1$", r"$\theta_2$", r"$\theta_3$"]
fig = corner.corner(samples, quantiles=[0.16, 0.5, 0.84], bins = nbr_bins,
                       show_titles=True, title_kwargs={"fontsize": 19}, labels=latex_labels)

### save file ###
filename = f'theta_corner_plot_task3.pdf'
plt.savefig(filename)
plt.show()


### plot best fit together with data ###
theta = np.sum(samples, axis=0) / len(samples) # use mean estimate
fig, ax = plt.subplots(figsize=(8,4))
ax.errorbar(alphas, mean_energies, yerr=2 * sigmas,
                            capsize=3.0,
                            linestyle='',
                            marker='o',
                            color='C0',
                            zorder=1,
                            label=r'mean $E(\alpha) \pm 2 \sigma$')
ax.plot(x_vec, predict_y(x_vec, theta), color='C1',
                                    zorder=2,
                                    label=r'mean estimate')
ax.set_xlabel(r'$\alpha$')
ax.set_ylabel(r'[a.u.]')

### legend & save file ###
plt.legend()
plt.tight_layout()
filename = f'plot_best_fit_to_data_task3.pdf'
plt.savefig(filename)
plt.show()


### plot pdf of y_min & x_min ###
energy_min = samples[:,0] # [a.u.]
alpha_min = (-samples[:,2] + np.sqrt(samples[:,2]**2 - 3 * samples[:,1] * samples[:,3])) / (3 * samples[:,3]) # the minimum of a 3rd degree polynomial a + bx + cx^2 + dx^3 occurs at x = (c +- sqrt(c^2 - 3bd))/(3d)
fig, (ax1, ax2) = plt.subplots(ncols=2, nrows=1, figsize=(8,4))
ax1.hist(energy_min, bins=50, color='C0', density=True)
ax1.axvline(np.mean(energy_min), color='C1', label=r'mean')
ax1.axvline(np.mean(energy_min) + np.sqrt(np.var(energy_min)), color='C1', linestyle='--', label=r'$\pm 1 \sigma$')
ax1.axvline(np.mean(energy_min) - np.sqrt(np.var(energy_min)), color='C1', linestyle='--')
ax1.set_xlabel(r'$E_\mathrm{min}$')
ax1.set_ylabel(r'$p(E_\mathrm{min}$)')
ax1.set_yticks([])
ax2.hist(alpha_min, bins=50, color='C0', density=True)
ax2.axvline(np.mean(alpha_min), color='C1', label=r'mean')
ax2.axvline(np.mean(alpha_min) + np.sqrt(np.var(alpha_min)), color='C1', linestyle='--', label=r'$\pm 1 \sigma$')
ax2.axvline(np.mean(alpha_min) - np.sqrt(np.var(alpha_min)), color='C1', linestyle='--')
ax2.set_xlabel(r'$\alpha_\mathrm{min}$')
ax2.set_ylabel(r'$p(\alpha_\mathrm{min}$)')
ax2.set_yticks([])
print(f'mean estimate of energy_min is {np.mean(energy_min):.6f} +- {np.sqrt(np.var(energy_min)):.6f}')
print(f'mean estimate of alpha_min is {np.mean(alpha_min):.6} +- {np.sqrt(np.var(alpha_min)):.6f}')

### legend & save file ###
ax1.legend()
ax2.legend()
plt.tight_layout()
filename = f'energy_and_alpha_pdfs_task3.pdf'
plt.savefig(filename)
plt.show()

