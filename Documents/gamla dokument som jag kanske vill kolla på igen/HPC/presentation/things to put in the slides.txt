My plan is to use x86 intrinsics to further optimize assignment 2. Specifically the 3D distance function.

By using the perf tool on my program, I can see that alot of time is spent calculating the distances between my 3-vectors, which are implemented using 16-bit integers but I will for this presentation consider an implementation using 32-bit integers since there is more architecture for that. This method is executed something like ordo(nbr_cells^2). So the question is, does the distance function lend itself to vector intrinsics?

The method consists of three arithmetic operations, +, -, and *. The + is "horizontal" while the - and * are "vertical" with respect to the 3-vectors. Also, a conversion to float is performed since we are not that picky about about precision and the float sqrtf method is much faster than the integer square root method.

The first approach is vectorizing the operations along the 3-vector. This is fairly easy to implement and is possible to implement without modifying the behaviour of the distance function. So if this would be a speed-up it would be very portable. The downside is the outlook, it's probably not efficient since 3 is an odd number, we'd have to pack it into a 4-vector, constantly discarding the last element.

With this approach, we get to utilize two intrinsic SIMD instructions, for - and for *. Since the register is 128-bit (hence 128i, i for int), we can pack 4 32-bit integers in there (hence the p in epi). The top one, for example, compiles directly to the assembler command PSUBD (and some MOV) which does indeed subtract two vectors of 4 32-bit integers. So then I implemented these in C and benchmarked them with Celero

Here is the result from 10 runs on Gantenbein. The for-loop is just a normal implementation without any intrinsics. It's fairly conclusive that this approach is not a winning strategy.

The second approach is more promising though. If we vectorize along the list of 3-vectors, we won't have the same problem with underpacking as with the first approach. This approach does although force us to rewrite code outside the distance function too in order to account for the operations being performed in batches. Also, since the list of 3-vectors in assignment 2 stores them as columns, we will have to perform a transpose in order to get to the vectors we want to operate on. But this approach is still much more promising than the first one.

Mainly, I think, due to this giving us three more operations that can be vectorized: +, conversion, and the square root. The s in ps stands for short, i.e. a 32-bit float.

And we see in the results from 10 runs on Gantenbein that we indeed get a speed-up. Around 35 %. I did try some other sizes also to see what would happen. My hypothesis was that since we already filled the registers to the brim, the only thing affecting the speed would be the transpose which is ordo(n^2). But the last one here goes down, which I have no good explanation for. So I guess that's why it's important to just try out different stuff and see what's fastest.

In conclusion. The different approaches performed sort of as expected and we managed to find one that was faster than the naive implementation. This could sadly not have been used to its full potential in assignment 2 since there one significant speed-up came from realizing we only needed 16-bit integers to represent our fixed-point numbers and there does not seem to exist the same intrinsic functions for 8 packed 16-bit integers.




Gurra said 'Typ prata mkt och snabbt' and 'Men om du har med assemberkod och pratar om pipelinen Tror jag det blir en 5a'. Gurra delade sin ppt och sina anteckningar med mig O:).
